name: CI - Go (Repo)

on:
  push:
    branches: [main]
  pull_request:

permissions: {}

jobs:
  build-artifact:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      # Use a run-scoped artifact name to prevent collisions between parallel CI runs.
      ARTIFACT_NAME: ksail-${{ github.run_id }}
    outputs:
      artifact-name: ${{ steps.publish.outputs.artifact-name }}
      checksum: ${{ steps.publish.outputs.checksum }}
      metrics: ${{ steps.publish-metrics.outputs.metrics }}
    steps:
      - name: ‚è±Ô∏è Record metrics start
        id: metrics-start
        run: echo "unix=$(date +%s)" >> "$GITHUB_OUTPUT"

      - name: üìÑ Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          persist-credentials: false
          fetch-depth: 0

      - name: ‚öôÔ∏è Setup Go
        id: setup-go
        uses: actions/setup-go@44694675825211faa026b3c33043df3e48a5fa00 # v6.0.0
        with:
          go-version-file: src/go.mod
            # Consistent cache path keeps module and build caches warm across all jobs.
          cache: true
          cache-dependency-path: src/go.sum

      - name: üì¶ Download dependencies
        working-directory: ./src/
        run: go mod download

      - name: üèóÔ∏è Build ksail binary
        run: go build -C src -o ../ksail .

      - name: üß™ Smoke test
        run: ./ksail --version

      - name: üîê Capture checksum
        id: checksum
        run: |
          sha=$(shasum -a 256 ksail | awk '{print $1}')
          echo "sha256=$sha" >> "$GITHUB_OUTPUT"

      - name: üì§ Upload artifact
        id: upload
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: ${{ env.ARTIFACT_NAME }}
          path: ksail
          if-no-files-found: error

      - name: üì¶ Publish outputs
        id: publish
        env:
          ARTIFACT_NAME: ${{ env.ARTIFACT_NAME }}
          SHA256: ${{ steps.checksum.outputs.sha256 }}
        run: |
          echo "artifact-name=${ARTIFACT_NAME}" >> "$GITHUB_OUTPUT"
          echo "checksum=${SHA256}" >> "$GITHUB_OUTPUT"

      - name: üìù Publish metrics
        id: publish-metrics
        if: always()
        run: METRICS_OUTPUT="$GITHUB_OUTPUT" ./.github/scripts/collect-metrics.sh
        env:
          METRICS_START_TIME: ${{ steps.metrics-start.outputs.unix }}
          METRICS_CACHE_HIT: ${{ steps.setup-go.outputs.cache-hit }}
          METRICS_ARTIFACT_CHECKSUM: ${{ steps.publish.outputs.checksum }}
          METRICS_JOB_NAME: build-artifact

  pre-commit:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    outputs:
      metrics: ${{ steps.publish-metrics.outputs.metrics }}
    steps:
      - name: ‚è±Ô∏è Record metrics start
        id: metrics-start
        run: echo "unix=$(date +%s)" >> "$GITHUB_OUTPUT"

      - name: üìÑ Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          persist-credentials: false
          fetch-depth: 0

      - name: ‚öôÔ∏è Setup Go
        id: setup-go
        uses: actions/setup-go@44694675825211faa026b3c33043df3e48a5fa00 # v6.0.0
        with:
          go-version-file: src/go.mod
          # Mirrors the build job cache key so downstream jobs reuse warmed modules.
          cache: true
          cache-dependency-path: src/go.sum

      - name: üîß Install mockery
        run: go install github.com/vektra/mockery/v3@latest

      - name: üèÉ Validate pre-commit hooks
        uses: pre-commit/action@2c7b3805fd2a0fd8c1884dcaebf91fc102a13ecd # v3.0.1
      - uses: pre-commit-ci/lite-action@5d6cc0eb514c891a40562a58a8e71576c5c7fb43 # v1.1.0
        if: always()

      - name: üìù Publish metrics
        id: publish-metrics
        if: always()
        run: METRICS_OUTPUT="$GITHUB_OUTPUT" ./.github/scripts/collect-metrics.sh
        env:
          METRICS_START_TIME: ${{ steps.metrics-start.outputs.unix }}
          METRICS_CACHE_HIT: ${{ steps.setup-go.outputs.cache-hit }}
          METRICS_ARTIFACT_CHECKSUM: n/a
          METRICS_JOB_NAME: pre-commit

  ci:
    needs: [pre-commit]
    uses: devantler-tech/reusable-workflows/.github/workflows/ci-go.yaml@1235788684c6576098528fd79c1c1e5a01ff3030 # v1.17.1
    with:
      working-directory: ./src/
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    permissions:
      contents: write
      issues: write
      packages: read
      pull-requests: write

  system-test:
    runs-on: ubuntu-latest
    # Run system tests on both push and pull request, but only after pre-commit and ci jobs complete
    needs: [ci, build-artifact]
    if: always() && (needs.ci.result == 'success' || needs.ci.result == 'skipped')
    strategy:
      matrix:
        include:
          - init-args: "--distribution Kind"
          - init-args: "--distribution Kind --cni Cilium"
          - init-args: "--distribution Kind --cni Calico"
          - init-args: "--distribution Kind --mirror-registry docker.io=https://registry-1.docker.io"
            test-workload: true
          - init-args: "--distribution Kind --mirror-registry docker.io=https://registry-1.docker.io --mirror-registry ghcr.io=https://ghcr.io"
            test-workload: true
          - init-args: "--distribution K3d"
          - init-args: "--distribution K3d --cni Cilium"
          - init-args: "--distribution K3d --cni Calico"
          - init-args: "--distribution K3d --mirror-registry docker.io=https://registry-1.docker.io"
            test-workload: true
          - init-args: "--distribution K3d --mirror-registry docker.io=https://registry-1.docker.io --mirror-registry ghcr.io=https://ghcr.io"
            test-workload: true
          - init-args: "--distribution K3d --metrics-server Disabled"
    steps:
      - name: ‚è±Ô∏è Record metrics start
        id: metrics-start
        run: echo "unix=$(date +%s)" >> "$GITHUB_OUTPUT"

      - name: üîí Verify build artifact
        if: ${{ needs.build-artifact.result != 'success' }}
          # Guard matrix execution when the build job fails, preserving runner minutes.
        run: |
          echo "Build artifact unavailable. Failing system-test matrix." >> "$GITHUB_STEP_SUMMARY"
          exit 1

      - name: üìÑ Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          persist-credentials: false

      - name: ‚öôÔ∏è Setup Go
        id: setup-go
        uses: actions/setup-go@44694675825211faa026b3c33043df3e48a5fa00 # v6.0.0
        with:
          go-version-file: ./src/go.mod
          cache: true
          cache-dependency-path: ./src/go.sum

      - name: üì• Prepare ksail artifact
        uses: ./.github/actions/use-ksail-artifact
        with:
          artifact-name: ${{ needs.build-artifact.outputs.artifact-name }}
          artifact-checksum: ${{ needs.build-artifact.outputs.checksum }}
          download-path: ./bin
          binary-name: ksail

      - name: üß™ ksail cluster init
        run: |
          ./bin/ksail cluster init ${{ matrix.init-args }}

      - name: üß™ ksail cluster create
        run: |
          ./bin/ksail cluster create

      - name: üß™ ksail cluster list
        run: |
          ./bin/ksail cluster list

      - name: üß™ ksail workload create
        if: matrix.test-workload
        run: |
          ./bin/ksail workload create deployment whoami --image=traefik/whoami:latest
          ./bin/ksail workload wait --for=condition=Available deployment/whoami --timeout=120s

      - name: üß™ ksail workload apply
        if: matrix.test-workload
        run: |
          mkdir -p /tmp/podinfo-overlay
          cat > /tmp/podinfo-overlay/kustomization.yaml <<'EOF'
          apiVersion: kustomize.config.k8s.io/v1beta1
          kind: Kustomization
          resources:
            - https://github.com/stefanprodan/podinfo//kustomize
          patches:
            - target:
                kind: HorizontalPodAutoscaler
              patch: |-
                $patch: delete
                apiVersion: autoscaling/v2
                kind: HorizontalPodAutoscaler
                metadata:
                  name: podinfo
          EOF
          ./bin/ksail workload apply -k /tmp/podinfo-overlay
          ./bin/ksail workload wait --for=condition=Available deployment/podinfo --timeout=300s

      - name: üß™ ksail workload reconcile
        run: |
          ./bin/ksail workload reconcile

      - name: üß™ ksail cluster stop
        run: |
          ./bin/ksail cluster stop

      - name: üß™ ksail cluster start
        run: |
          ./bin/ksail cluster start

      - name: üß™ ksail cluster delete
        run: |
          ./bin/ksail cluster delete

      - name: üßπ Cleanup
        run: |
          if [ -d "k8s" ]; then rm -rf "k8s"; fi
          if [ -f "kind.yaml" ]; then rm "kind.yaml"; fi
          if [ -f "k3d.yaml" ]; then rm "k3d.yaml"; fi
          if [ -f "ksail.yaml" ]; then rm "ksail.yaml"; fi

      - name: üìù Publish metrics
        id: publish-metrics
        if: always()
        run: METRICS_OUTPUT="$GITHUB_OUTPUT" ./.github/scripts/collect-metrics.sh
        env:
          METRICS_START_TIME: ${{ steps.metrics-start.outputs.unix }}
          METRICS_CACHE_HIT: ${{ steps.setup-go.outputs.cache-hit }}
          METRICS_ARTIFACT_CHECKSUM: ${{ needs.build-artifact.outputs.checksum }}
          METRICS_JOB_NAME: system-test-${{ matrix.init-args }}

  system-test-status:
    runs-on: ubuntu-latest
    needs: [system-test, build-artifact]
    if: always()
    outputs:
      metrics: ${{ steps.publish-metrics.outputs.metrics }}
    steps:
      - name: ‚è±Ô∏è Record metrics start
        id: metrics-start
        run: echo "unix=$(date +%s)" >> "$GITHUB_OUTPUT"

      - name: Verify artifact availability
        if: needs.build-artifact.result != 'success'
          # Short-circuit the status job so upstream artifact failures surface quickly.
        run: |
          echo "Build artifact unavailable. Failing status aggregation." >> "$GITHUB_STEP_SUMMARY"
          exit 1

      - name: Summarize matrix result
        shell: bash
        run: |
          set -Eeuo pipefail
          echo "system-test result: $MATRIX_RESULT"
          case "$MATRIX_RESULT" in
            success|skipped)
              echo "All matrix runs succeeded or were skipped."
              exit 0
              ;;
            *)
              echo "At least one matrix run failed."
              exit 1
              ;;
          esac
        env:
          MATRIX_RESULT: ${{ needs.system-test.result }}

      - name: üìù Publish metrics
        id: publish-metrics
        if: always()
        run: METRICS_OUTPUT="$GITHUB_OUTPUT" ./.github/scripts/collect-metrics.sh
        env:
          METRICS_START_TIME: ${{ steps.metrics-start.outputs.unix }}
          METRICS_CACHE_HIT: n/a
          METRICS_ARTIFACT_CHECKSUM: ${{ needs.build-artifact.outputs.checksum }}
          METRICS_JOB_NAME: system-test-status

  metrics-summary:
    runs-on: ubuntu-latest
    needs:
      - build-artifact
      - pre-commit
      - ci
      - system-test
      - system-test-status
    if: always()
    steps:
      - name: Aggregate CI metrics
        run: |
          export BUILD_METRICS='${{ needs.build-artifact.outputs.metrics }}'
          export PRECOMMIT_METRICS='${{ needs.pre-commit.outputs.metrics }}'
          export SYSTEM_STATUS_METRICS='${{ needs.system-test-status.outputs.metrics }}'
          export SYSTEM_RESULT='${{ needs.system-test.result }}'

          python3 <<'PY'
          import json
          import os

          summary_lines = ["### CI Performance Snapshot"]

          def parse_json(value):
            if not value or value == '':
              return None
            try:
              return json.loads(value)
            except json.JSONDecodeError:
              return None

          def add_metric(title, data):
            if not data:
              summary_lines.append(f"- {title}: unavailable")
              return
            summary_lines.append(
              f"- {title}: {data.get('durationSeconds', 'n/a')}s (cache={data.get('cacheStatus', 'n/a')}, sha={data.get('artifactChecksum', 'n/a')})"
            )

          build = parse_json(os.environ.get('BUILD_METRICS'))
          precommit = parse_json(os.environ.get('PRECOMMIT_METRICS'))
          status = parse_json(os.environ.get('SYSTEM_STATUS_METRICS'))

          add_metric("build-artifact", build)
          add_metric("pre-commit", precommit)

          if status:
            summary_lines.append(
              f"- system-test status: {status.get('durationSeconds', 'n/a')}s (cache={status.get('cacheStatus', 'n/a')}, sha={status.get('artifactChecksum', 'n/a')})"
            )
          else:
            summary_lines.append(
              f"- system-test status: unavailable (result={os.environ.get('SYSTEM_RESULT', 'unknown')})"
            )

          lint_cache = (os.environ.get('LINT_CACHE_HIT') or 'unknown') or 'unknown'
          test_cache = (os.environ.get('TEST_CACHE_HIT') or 'unknown') or 'unknown'
          summary_lines.append(f"- lint cache hit: {lint_cache}")
          summary_lines.append(f"- test cache hit: {test_cache}")

          with open(os.environ.get('GITHUB_STEP_SUMMARY'), 'a', encoding='utf-8') as fh:
            fh.write("\n".join(summary_lines) + "\n")
          PY
