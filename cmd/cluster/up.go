package cluster

import (
	"context"
	"fmt"
	"os"
	"time"

	"github.com/devantler-tech/ksail-go/cmd/internal/cmdhelpers"
	"github.com/devantler-tech/ksail-go/pkg/apis/cluster/v1alpha1"
	k3dconfig "github.com/devantler-tech/ksail-go/pkg/config-manager/k3d"
	kindconfig "github.com/devantler-tech/ksail-go/pkg/config-manager/kind"
	configmanager "github.com/devantler-tech/ksail-go/pkg/config-manager/ksail"
	clusterprovisioner "github.com/devantler-tech/ksail-go/pkg/provisioner/cluster"
	k3dprovisioner "github.com/devantler-tech/ksail-go/pkg/provisioner/cluster/k3d"
	kindprovisioner "github.com/devantler-tech/ksail-go/pkg/provisioner/cluster/kind"
	"github.com/devantler-tech/ksail-go/pkg/provisioner/containerengine"
	"github.com/devantler-tech/ksail-go/pkg/ui/notify"
	"github.com/devantler-tech/ksail-go/pkg/ui/timer"
	"github.com/spf13/cobra"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/kind/pkg/cluster"
)

const defaultUpTimeout = 5 * time.Minute

// NewUpCmd creates and returns the up command.
func NewUpCmd() *cobra.Command {
	cmd := cmdhelpers.NewCobraCommand(
		"up",
		"Start the Kubernetes cluster",
		`Start the Kubernetes cluster defined in the project configuration.`,
		HandleUpRunE,
		cmdhelpers.StandardDistributionFieldSelector(),
		cmdhelpers.StandardDistributionConfigFieldSelector(),
		cmdhelpers.StandardContextFieldSelector(),
		configmanager.FieldSelector[v1alpha1.Cluster]{
			Selector:     func(c *v1alpha1.Cluster) any { return &c.Spec.Connection.Timeout },
			Description:  "Timeout for cluster operations",
			DefaultValue: metav1.Duration{Duration: defaultUpTimeout},
		},
	)

	// Add force flag for delete/recreate semantics
	cmd.Flags().Bool("force", false, "Force delete and recreate cluster if it already exists")

	return cmd
}

// HandleUpRunE handles the up command.
// Exported for testing purposes.
func HandleUpRunE(
	cmd *cobra.Command,
	manager *configmanager.ConfigManager,
	_ []string,
) error {
	ctx := context.Background()

	// Start timing
	tmr := timer.New()

	// Load configuration (already implements precedence: flags â†’ env â†’ files â†’ defaults)
	config, err := manager.LoadConfig()
	if err != nil {
		return fmt.Errorf("failed to load cluster configuration: %w", err)
	}

	// Check if force flag is set (pass empty string to use config name)
	force, _ := cmd.Flags().GetBool("force")

	// Add section header for provisioning
	fmt.Fprintln(manager.Writer)
	notify.TitleMessage(manager.Writer, "ðŸš€", notify.NewMessage("Provisioning cluster..."))

	// Check dependencies before provisioning
	tmr.StartStage()
	notify.ActivityMessage(manager.Writer, notify.NewMessage("checking dependencies"))
	err = checkDependencies(config)
	if err != nil {
		notify.ErrorMessage(
			manager.Writer,
			notify.NewMessage(
				fmt.Sprintf("dependency check failed after %s", tmr.Total().Round(time.Second)),
			),
		)
		notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("Error: %v", err)))
		return fmt.Errorf("dependency check failed: %w", err)
	}
	notify.SuccessMessage(
		manager.Writer,
		notify.NewMessage("dependencies checked").WithTiming(tmr.Total(), tmr.Stage()),
	)

	// Wire the correct provisioner and config manager
	tmr.StartStage()
	notify.ActivityMessage(manager.Writer, notify.NewMessage("setting up provisioner"))
	provisioner, err := createProvisioner(config)
	if err != nil {
		notify.ErrorMessage(
			manager.Writer,
			notify.NewMessage(
				fmt.Sprintf(
					"failed to create provisioner after %s",
					tmr.Total().Round(time.Second),
				),
			),
		)
		notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("Error: %v", err)))
		return fmt.Errorf("failed to create provisioner: %w", err)
	}

	// Handle force recreation or idempotent reuse
	exists, err := provisioner.Exists(ctx, "")
	if err != nil {
		notify.ErrorMessage(
			manager.Writer,
			notify.NewMessage(
				fmt.Sprintf(
					"failed to check cluster existence after %s",
					tmr.Total().Round(time.Second),
				),
			),
		)
		notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("Error: %v", err)))
		return fmt.Errorf("failed to check cluster existence: %w", err)
	}

	if exists && force {
		notify.ActivityMessage(
			manager.Writer,
			notify.NewMessage("force flag set, deleting existing cluster"),
		)
		err = provisioner.Delete(ctx, "")
		if err != nil {
			notify.ErrorMessage(
				manager.Writer,
				notify.NewMessage(
					fmt.Sprintf(
						"failed to delete cluster after %s",
						tmr.Total().Round(time.Second),
					),
				),
			)
			notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("Error: %v", err)))
			return fmt.Errorf("failed to delete cluster during force recreation: %w", err)
		}
		exists = false
	}

	if !exists {
		notify.ActivityMessage(manager.Writer, notify.NewMessage("creating cluster"))
		err = provisioner.Create(ctx, "")
		if err != nil {
			notify.ErrorMessage(
				manager.Writer,
				notify.NewMessage(
					fmt.Sprintf(
						"failed to create cluster after %s",
						tmr.Total().Round(time.Second),
					),
				),
			)
			notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("Error: %v", err)))
			return fmt.Errorf("failed to create cluster: %w", err)
		}
	} else {
		notify.ActivityMessage(manager.Writer, notify.NewMessage("cluster already exists, starting"))
		err = provisioner.Start(ctx, "")
		if err != nil {
			notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("failed to start cluster after %s", tmr.Total().Round(time.Second))))
			notify.ErrorMessage(manager.Writer, notify.NewMessage(fmt.Sprintf("Error: %v", err)))
			return fmt.Errorf("failed to start cluster: %w", err)
		}
	}

	// Wait for readiness
	tmr.StartStage()
	notify.ActivityMessage(manager.Writer, notify.NewMessage("waiting for cluster readiness"))

	// Placeholder for readiness polling - will be implemented in T008
	// For now, mark as ready immediately to pass basic tests

	// Merge kubeconfig and set context
	notify.ActivityMessage(
		manager.Writer,
		notify.NewMessage(
			fmt.Sprintf("updating kubeconfig at %s", config.Spec.Connection.Kubeconfig),
		),
	)

	// Placeholder for kubeconfig merge - will be implemented in T008
	// Kind/K3d provisioners already update kubeconfig automatically

	// Emit success output with timing
	notify.SuccessMessage(
		manager.Writer,
		notify.NewMessage("cluster is ready").WithTiming(tmr.Total(), tmr.Stage()),
	)

	return nil
}

// checkDependencies verifies prerequisites are met for the selected distribution
func checkDependencies(config *v1alpha1.Cluster) error {
	switch config.Spec.Distribution {
	case v1alpha1.DistributionKind, v1alpha1.DistributionK3d:
		// Check Docker/Podman availability
		engine, err := containerengine.GetAutoDetectedClient()
		if err != nil {
			return fmt.Errorf(
				"container engine (Docker/Podman) required but not available: %w\nPlease start Docker or Podman and rerun 'ksail cluster up'",
				err,
			)
		}
		if engine == nil {
			return fmt.Errorf(
				"no container engine detected\nPlease install and start Docker or Podman, then rerun 'ksail cluster up'",
			)
		}
	}
	return nil
}

// createProvisioner instantiates the correct provisioner based on distribution
func createProvisioner(config *v1alpha1.Cluster) (clusterprovisioner.ClusterProvisioner, error) {
	switch config.Spec.Distribution {
	case v1alpha1.DistributionKind:
		// Load Kind-specific configuration
		kindConfigManager := kindconfig.NewConfigManager(config.Spec.DistributionConfig, os.Stdout)
		kindConfig, err := kindConfigManager.LoadConfig()
		if err != nil {
			return nil, fmt.Errorf("failed to load Kind configuration: %w", err)
		}

		// Wire real Kind provisioner with cluster.Provider adapter and Docker client
		provider := kindprovisioner.NewKindProviderAdapter(cluster.NewProvider())
		engine, err := containerengine.GetAutoDetectedClient()
		if err != nil {
			return nil, fmt.Errorf("failed to get container engine client: %w", err)
		}

		return kindprovisioner.NewKindClusterProvisioner(
			kindConfig,
			config.Spec.Connection.Kubeconfig,
			provider,
			engine.Client,
		), nil

	case v1alpha1.DistributionK3d:
		// Load K3d-specific configuration
		k3dConfigManager := k3dconfig.NewConfigManager(config.Spec.DistributionConfig, os.Stdout)
		k3dConfig, err := k3dConfigManager.LoadConfig()
		if err != nil {
			return nil, fmt.Errorf("failed to load K3d configuration: %w", err)
		}

		// Wire K3d provisioner with adapters
		clientAdapter := k3dprovisioner.NewK3dClientAdapter()
		configAdapter := k3dprovisioner.NewK3dConfigAdapter()

		return k3dprovisioner.NewK3dClusterProvisioner(
			k3dConfig,
			clientAdapter,
			configAdapter,
		), nil

	default:
		return nil, fmt.Errorf("unsupported distribution: %s", config.Spec.Distribution)
	}
}
